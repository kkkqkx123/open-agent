llm分组：
主模型[大模型]：基于任务类型(fast, plan, thinking, execute, review, high-payload[支持高并发])+echelon划分【任务类型+水平】
小模型：基于水平、任务类型(fast[最简单的任务], translation[翻译], analysis, execute[工具调用/json格式化输出], thinking, high-payload)
基于节点设置模型分组与降级方案，移除按照llm设置降级
支持配置每个分组的并发数、RPM

允许节点配置llm组，并兼容配置单独的llm

单轮对话任务可以使用轮询池。轮询池采用与之前llm分组类似的组划分

---
工具调用输出：使用纯文本，不使用json

---

命令执行：
命令执行结果如果超出一定行数，先由小模型处理，再给大模型

代码搜索：
根据召回+重排的结果+整理最终片段，小模型读取片段及完整文件，判断哪些内容应当交给大模型，文件给出相对路径即可

搜索/看文档：
__

逻辑推理：
识别原子命题，以形式逻辑的形式整理为主析取/合取
自然语言的关系逻辑转3元组

---
路径召回：根据任务需要尝试多种匹配模式，直接找出所有文件/路径的相对路径作为初始上下文
---

工具：
回滚文件修改工具

提示词：
类型检查、单元测试优先于执行demo/集成测试/e2e测试

injection提示词：用于破限

多种工具调用格式及配套提示词

所有工作流都要给出demo，放在workflow_demo目录中

系统提示词：尽量在项目根目录执行命令，不要cd到别的目录
检查导入时只需要查看文件的头几行【延迟导入除外】

监控：监控启动的终端的pid，实时更新，通过slash命令关闭特定/所有pid，并提供一键关闭脚本

AGENT：scout
用qwen2.5-7b或Qwen/Qwen2.5-Coder-7B-Instruct或THUDM/glm-4-9b-chat【硅基】或GLM-4.5-Flash【Zai】模型根据重排结果、相应文件内容对文件与问题的联系进行分析，输出摘要，
然后把重排结果、摘要汇总给ds-r1-0528-8b，给出最后的摘要。

搜索：
给出多种匹配模式，并根据返回调整(如过滤文档、测试文件)

测试专家：
执行测试文件。如果是原文件的逻辑存在问题，则结束并给出总结，返回给主agent


会话增加fork功能，复制会话信息【只持久化，不必马上打开】


分析：每次更新todo list时都要输出针对该步骤的思考，为后续思考提供上下文


新增hook功能：
输入错误检查与修复：例如任务输成人物。【中文主要是同音，英语主要是拼写错误。只检查用户指令。需要过滤掉命令返回等信息】直接修改即可


代码审计：codeQL/semgrep


输入信息(仅用户输入的语言表达部分，不关注'''块/json/xml等)语病检查


上下文压缩等价于fork


每次响应都要使用工具【除非是专门的分析节点】

文件操作：
记录修改过程，可以通过工具回退
如果修改引入了格式错误，则可以回退到上一次正确的版本

git专注于储存每个session开始前的原始
在git的基础上新增每个修改过的文件的原始状态的持久化存储(需要压缩)，提供回退thread生命周期内对文件的修改的功能

诊断工具：pylance、ts lsp【可以针对文件/目录】

重写文件不要搜索与替换。直接覆盖(override工具)

新增工具：领域知识查询。可以查询特定领域的知识及工具调用指南，作为特定任务下动态导入的上下文


无缝衔接human relay模式。
支持配置每个节点【主要是那些需要干净上下文或不需要完整上下文的】是否使用human relay模式。
例如context7 mcp搜索节点可以设置为human relay模式

增加重构模式。提示词中要求ai始终采用更根本的方式解决问题，不要放任架构问题不管

debug：执行修改前必须先考虑清楚修改的后果，并分析是否有其他的方案。例如类型不符时分别考虑修改哪个文件是更好的选择。

创建todo list后由小模型检查是否符合要求(可以在计划阶段要求llm通过记忆工具写入分析结果，作为上下文)，
任务执行完后由思考大模型验证是否完成了所有的要求

终端工具：
添加可选参数：获取终端输出信息的间隔。过长信息截断。终端信息先由小模型处理，再给大模型


cli实现与tui类似的功能。输入、模型输出、工具调用结果直接在终端输出

cli新增速率预检测，根据provider-llm分级的请求速率(单独设置RPM,TPM)作出检查，
判断当前配置文件中实际使用的控制参数是否会超出限制(可以假定请求间隔)

限制最大轮数时最后几轮提示ai并要求结束任务。也支持添加提醒开始，方便更好的控制对话轮数

完全的流式输出改为按段输出

编写脚本，用于清理从TUI界面复制出来的包含ui特殊字符的文本

新思路：方案审核节点：干净的上下文+修改方案分段后逐段寻找召回片段，仅最终聚合时使用scout小模型，从而保证上下文尽可能独立

结束任务工具：触发hook，用于激活监督节点，避免全局连接节点

代码静态检查工具：直接使用black, mypy, isort。如果不存在，要求用户批准安装
semgrep，定义导入顺序等规则

编辑文件工具：diff、search and replace、override、regex【基于正则模式替换】[首次使用返回替换内容预览(ripgrep)，让llm确认，确认后才执行]

undo_edit工具

检查点可以参考ccundo

文件系统工具分析
初级：仅完整读取、从头/从尾读取单个文件+重写文件
中级：
高级：



存档点工具：执行危险操作前自动内联该工具，先存档再执行危险工具调用

错误大量增加：直接转人工检查/llm审核【给出diff[需要带一定上下文]，判断是否犯了低级错误，如方法插入方法内部】

铃声支持【作业阻塞、人工节点、作业完成、api调用失败(包括降级方案失败)】

---
上下文压缩必须保留原先上下文，避免回退到压缩前时上下文丢失

上下文剪枝工作流：压缩上下文时可以启用【可以通过配置设置是否启用】
支持配置每个子段划分时使用的上文子段数量、RAG召回上下文数量【可以禁用】
为每个工具新增可选配置项：压缩配置。例如仅保留哪段使用/返回时的json字段
上下文类别映射：例如文件内容、用于指导小模型判断是否需要压缩。

注意：仅压缩当前工作流信息

### 预处理阶段
1.提取关键节点：提取用户指令、todo list更新、工作流切换[关注调用、结束、回调的元信息及结束任务时的llm响应]、llm使用结束任务工具(在工具执行节点捕获)、工作流结束时的元信息【这些节点的完整信息必须保留[用户指令、todo list更新]】，且必须保留完整最后一个用户输入、最后的todo list
一个工作流中使用了同步agent执行的要分别处理(例如ultra thinking)
2.分段：基于关键节点分段，再基于每次llm节点的输入、输出划分子段
3.子段压缩：
-用分词器[tiktoken]计算每一个子段的token，对照上下文类别映射，逐段使用小模型决定保留/压缩(直接json格式化输出，不需要工具调用)【可以高并发，建议使用高并发小模型分组】。对于token小于指定值的子段，如果小模型判断保留可以直接接受，判断压缩或token超限则直接下一步
-长于指定值的由llm压缩(提示词中说明对于特定问题可以选择不压缩/要求更多上下文信息。分支包括：给出压缩结果、选择不压缩、要求更多上下文信息)。【可以根据长度动态选择不同类别的模型】。这一步必须顺序执行，因为包含上文时需要已压缩的上文。每完成一步，将这个字段添加向量嵌入索引
-检查每一段压缩是否合适：
	-如果压缩了：检验压缩后是否token数更多
	-检验压缩结果[包括不压缩，需要人为分析是否有必要压缩]可以设置是否由人工审核【分3种情况：通过、不通过、跳过压缩。不通过：给出压缩指示，重新压缩。旧的压缩上下文不保留】，也可以由小模型审核(提示词中说明要重点检查信息丢失是否可接受)或不审核【通过工作流配置管理】。
4.段压缩：
-用分词器计算每一段输入、输出的token
-长于指定值的由大模型压缩经过压缩过的每一段【可以根据长度动态选择不同类别的模型】[与子段压缩同理]
-检查每一段压缩是否合适。具体逻辑同上
5.最终压缩
-用分词器计算总token
-长于指定值的由llm压缩[同上]
-检查每一段压缩是否合适[同上]

---

完善记忆功能【区分工作流记忆、组记忆。每个工作流分别设置使用哪个组记忆】

---

提示词增强：同样采用RAG。可以结合代码库索引

---

单轮高性能模型对话/连续工具调用失败/上下文太长：
上下文剪枝+human relay

针对todo list判断工作流大小：由思考模型检查。如果太大，部分工作直接剔除，并回调todo list及修改意见

工作流：文件名检查

codemap：notebook+AST节点。
把代码内容和ast解析结果【仅提取实体】交给llm，要求llm把分析内容填在AST节点对应块中。
然后渲染notebook


终端工具：可以选择是否复用终端。如果复用，运行过程中终端始终挂载在后台，需要使用时执行cd命令
可以先静默运行pwd命令，确定目录的安全性
必须支持在内存泄漏时自行关闭，也必须可跟踪


工作流：fetch+文本清洗+总结相关内容


工具：缩进调整工具(适用于py)