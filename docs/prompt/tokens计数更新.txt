由于当前项目处于开发阶段，不需要考虑渐进式修改，可以直接过渡。

此外，需要修改方案文档，将新增的基于api返回体解析的tokens计算与之前的统计逻辑放在新的文件中，原先的src\llm\token_counter.py作为2种计算方式的协调器，以避免单个文件规模过大。

旧的tokens计算主要用于降级。当返回体解析得到的tokens数目少于本地tokens估算的1/4时，直接使用本地tokens计算，忽略api返回体的信息(这种情况大概率是上游api provider的计数有问题)

此外，当前的本地token缓存计算没有意义，直接移除。只需要从返回体中获取即可。
由于部分api provider不支持缓存，应当在模型设置中增加是否支持缓存的配置项。如果设置为不支持缓存或返回体中解析失败，则不需要更新缓存tokens计数

另外，分析当前设计的tokens计数在多轮对话的情况是否存在缺陷，并在方案中给出优化方案