#!/usr/bin/env python3
"""æµ‹è¯•å‚æ•°æ”¯æŒçš„ç®€å•è„šæœ¬"""

import sys
import os
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "src"))

def test_config_loading():
    """æµ‹è¯•é…ç½®åŠ è½½"""
    print("æµ‹è¯•é…ç½®åŠ è½½...")
    
    try:
        # ä¿®å¤å¯¼å…¥é—®é¢˜
        sys.path.insert(0, str(Path(__file__).parent))
        
        from llm.config import LLMClientConfig, OpenAIConfig, AnthropicConfig, GeminiConfig, MockConfig
        
        # æµ‹è¯•OpenAIé…ç½®
        openai_config_dict = {
            "model_type": "openai",
            "model_name": "gpt-4",
            "api_key": "test-key",
            "temperature": 0.7,
            "max_tokens": 2000,
            "max_completion_tokens": 4000,
            "frequency_penalty": 0.1,
            "presence_penalty": 0.1,
            "top_p": 0.9,
            "top_logprobs": 5,
            "tool_choice": "auto",
            "response_format": {"type": "json_object"},
            "service_tier": "flex",
            "safety_identifier": "test-user",
            "store": True,
            "reasoning": {"effort": "medium"},
            "verbosity": "high",
            "web_search_options": {"enabled": True},
            "seed": 42,
            "user": "test-user"
        }
        
        openai_config = OpenAIConfig.from_dict(openai_config_dict)
        print(f"âœ“ OpenAIé…ç½®åŠ è½½æˆåŠŸ: {openai_config.model_name}")
        print(f"  - max_completion_tokens: {openai_config.max_completion_tokens}")
        print(f"  - top_logprobs: {openai_config.top_logprobs}")
        print(f"  - service_tier: {openai_config.service_tier}")
        
        # æµ‹è¯•Anthropicé…ç½®
        anthropic_config_dict = {
            "model_type": "anthropic",
            "model_name": "claude-3-sonnet-20240229",
            "api_key": "test-key",
            "temperature": 0.7,
            "max_tokens": 2000,
            "top_p": 0.9,
            "top_k": 40,
            "stop_sequences": ["END", "STOP"],
            "system": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹",
            "thinking_config": {"thinking_budget": 1024},
            "tool_choice": "auto"
        }
        
        anthropic_config = AnthropicConfig.from_dict(anthropic_config_dict)
        print(f"âœ“ Anthropicé…ç½®åŠ è½½æˆåŠŸ: {anthropic_config.model_name}")
        print(f"  - top_k: {anthropic_config.top_k}")
        print(f"  - stop_sequences: {anthropic_config.stop_sequences}")
        print(f"  - thinking_config: {anthropic_config.thinking_config}")
        
        # æµ‹è¯•Geminié…ç½®
        gemini_config_dict = {
            "model_type": "gemini",
            "model_name": "gemini-pro",
            "api_key": "test-key",
            "temperature": 0.7,
            "max_tokens": 2000,
            "max_output_tokens": 4000,
            "top_p": 0.9,
            "top_k": 40,
            "stop_sequences": ["END"],
            "candidate_count": 1,
            "system_instruction": {"parts": [{"text": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹"}]},
            "response_mime_type": "application/json",
            "thinking_config": {"thinking_budget": 1024},
            "safety_settings": {"threshold": "BLOCK_NONE"}
        }
        
        gemini_config = GeminiConfig.from_dict(gemini_config_dict)
        print(f"âœ“ Geminié…ç½®åŠ è½½æˆåŠŸ: {gemini_config.model_name}")
        print(f"  - max_output_tokens: {gemini_config.max_output_tokens}")
        print(f"  - candidate_count: {gemini_config.candidate_count}")
        print(f"  - response_mime_type: {gemini_config.response_mime_type}")
        
        # æµ‹è¯•Mocké…ç½®
        mock_config_dict = {
            "model_type": "mock",
            "model_name": "mock-model",
            "temperature": 0.7,
            "max_tokens": 2000,
            "response_delay": 0.1,
            "error_rate": 0.0,
            "error_types": ["timeout", "rate_limit"]
        }
        
        mock_config = MockConfig.from_dict(mock_config_dict)
        print(f"âœ“ Mocké…ç½®åŠ è½½æˆåŠŸ: {mock_config.model_name}")
        print(f"  - response_delay: {mock_config.response_delay}")
        print(f"  - error_rate: {mock_config.error_rate}")
        
        return True
        
    except Exception as e:
        print(f"âœ— é…ç½®åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_config_files():
    """æµ‹è¯•é…ç½®æ–‡ä»¶"""
    print("\næµ‹è¯•é…ç½®æ–‡ä»¶...")
    
    try:
        import yaml
        
        # æµ‹è¯•OpenAIé…ç½®æ–‡ä»¶
        with open("configs/llms/openai-gpt4.yaml", 'r', encoding='utf-8') as f:
            openai_yaml = yaml.safe_load(f)
        
        print(f"âœ“ OpenAIé…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ")
        print(f"  - æ¨¡å‹: {openai_yaml['model_name']}")
        print(f"  - å‚æ•°æ•°é‡: {len(openai_yaml.get('parameters', {}))}")
        
        # æµ‹è¯•Anthropicé…ç½®æ–‡ä»¶
        with open("configs/llms/anthropic-claude.yaml", 'r', encoding='utf-8') as f:
            anthropic_yaml = yaml.safe_load(f)
        
        print(f"âœ“ Anthropicé…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ")
        print(f"  - æ¨¡å‹: {anthropic_yaml['model_name']}")
        print(f"  - å‚æ•°æ•°é‡: {len(anthropic_yaml.get('parameters', {}))}")
        
        # æµ‹è¯•Geminié…ç½®æ–‡ä»¶
        with open("configs/llms/gemini-pro.yaml", 'r', encoding='utf-8') as f:
            gemini_yaml = yaml.safe_load(f)
        
        print(f"âœ“ Geminié…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ")
        print(f"  - æ¨¡å‹: {gemini_yaml['model_name']}")
        print(f"  - å‚æ•°æ•°é‡: {len(gemini_yaml.get('parameters', {}))}")
        
        # æµ‹è¯•Mocké…ç½®æ–‡ä»¶
        with open("configs/llms/mock.yaml", 'r', encoding='utf-8') as f:
            mock_yaml = yaml.safe_load(f)
        
        print(f"âœ“ Mocké…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ")
        print(f"  - æ¨¡å‹: {mock_yaml['model_name']}")
        print(f"  - å‚æ•°æ•°é‡: {len(mock_yaml.get('parameters', {}))}")
        
        return True
        
    except Exception as e:
        print(f"âœ— é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_parameter_coverage():
    """æµ‹è¯•å‚æ•°è¦†ç›–åº¦"""
    print("\næµ‹è¯•å‚æ•°è¦†ç›–åº¦...")
    
    # OpenAIå‚æ•°åˆ—è¡¨
    openai_params = [
        'temperature', 'max_tokens', 'max_completion_tokens', 'top_p', 
        'frequency_penalty', 'presence_penalty', 'stop', 'top_logprobs',
        'tool_choice', 'tools', 'response_format', 'stream_options',
        'service_tier', 'safety_identifier', 'store', 'reasoning',
        'verbosity', 'web_search_options', 'seed', 'user'
    ]
    
    # Anthropicå‚æ•°åˆ—è¡¨
    anthropic_params = [
        'temperature', 'max_tokens', 'top_p', 'top_k', 'stop_sequences',
        'tool_choice', 'tools', 'system', 'thinking_config', 'response_format',
        'metadata', 'user'
    ]
    
    # Geminiå‚æ•°åˆ—è¡¨
    gemini_params = [
        'temperature', 'max_tokens', 'max_output_tokens', 'top_p', 'top_k',
        'stop_sequences', 'candidate_count', 'system_instruction',
        'response_mime_type', 'thinking_config', 'safety_settings',
        'tool_choice', 'tools', 'user'
    ]
    
    print(f"âœ“ OpenAIæ”¯æŒå‚æ•°æ•°é‡: {len(openai_params)}")
    print(f"âœ“ Anthropicæ”¯æŒå‚æ•°æ•°é‡: {len(anthropic_params)}")
    print(f"âœ“ Geminiæ”¯æŒå‚æ•°æ•°é‡: {len(gemini_params)}")
    
    return True

def main():
    """ä¸»å‡½æ•°"""
    print("å¼€å§‹éªŒè¯LLMå‚æ•°æ”¯æŒ...")
    
    success = True
    
    # æµ‹è¯•é…ç½®åŠ è½½
    success &= test_config_loading()
    
    # æµ‹è¯•é…ç½®æ–‡ä»¶
    success &= test_config_files()
    
    # æµ‹è¯•å‚æ•°è¦†ç›–åº¦
    success &= test_parameter_coverage()
    
    if success:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼LLMå‚æ•°æ”¯æŒå·²æˆåŠŸæ›´æ–°ã€‚")
        return 0
    else:
        print("\nâŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®ã€‚")
        return 1

if __name__ == "__main__":
    sys.exit(main())