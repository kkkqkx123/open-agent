# æç¤ºè¯ç¼“å­˜å†—ä½™æ€§åˆ†ææŠ¥å‘Š

## æ¦‚è¿°

æœ¬æŠ¥å‘Šåˆ†æå½“å‰é¡¹ç›®ä¸­æç¤ºè¯ç¼“å­˜çš„å†—ä½™æ€§å’Œå¿…è¦æ€§ï¼Œé‡ç‚¹å…³æ³¨é¡¹ç›®ä»…è°ƒç”¨LLM APIè€Œä¸ä½¿ç”¨LLMç¼“å­˜çš„å®é™…æƒ…å†µï¼Œä»¥åŠtokenç»Ÿè®¡æ¨¡å—æ— æ³•ä»APIå“åº”ä¸­è·å–ç¼“å­˜å‘½ä¸­ä¿¡æ¯çš„é—®é¢˜ã€‚

## å½“å‰æ¶æ„åˆ†æ

### 1. æç¤ºè¯ç¼“å­˜æ¶æ„æ¦‚è§ˆ

#### ç¼“å­˜å±‚æ¬¡ç»“æ„
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ åº”ç”¨å±‚ç¼“å­˜ (Tokenå¤„ç†å™¨)                                  â”‚
â”‚ - BaseTokenProcessor._usage_cache                        â”‚
â”‚ - HybridTokenProcessorç¼“å­˜                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æœåŠ¡å±‚ç¼“å­˜ (CacheManager)                                â”‚
â”‚ - MemoryCacheProvider                                    â”‚
â”‚ - GeminiServerCacheProvider                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ åŸºç¡€è®¾æ–½å±‚ç¼“å­˜ (LLMå®¢æˆ·ç«¯)                               â”‚
â”‚ - OpenAI/Gemini/Anthropicå®¢æˆ·ç«¯ç¼“å­˜                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### ä¸»è¦ç¼“å­˜ç»„ä»¶

1. **Tokenå¤„ç†å™¨ç¼“å­˜** (`src/services/llm/token_processing/`)
   - [`BaseTokenProcessor._usage_cache`](src/services/llm/token_processing/base_processor.py:307): Tokenä½¿ç”¨ç»“æœç¼“å­˜
   - [`HybridTokenProcessor`](src/services/llm/token_processing/hybrid_processor.py:175): æ··åˆç¼“å­˜ç­–ç•¥

2. **ç»Ÿä¸€ç¼“å­˜ç®¡ç†å™¨** (`src/core/llm/cache/`)
   - [`CacheManager`](src/core/llm/cache/cache_manager.py:15): ç»Ÿä¸€ç¼“å­˜ç®¡ç†
   - [`MemoryCacheProvider`](src/core/llm/cache/memory_provider.py:13): å†…å­˜ç¼“å­˜å®ç°
   - [`GeminiServerCacheProvider`](src/core/llm/cache/providers/gemini_server_provider.py:12): GeminiæœåŠ¡å™¨ç«¯ç¼“å­˜

3. **LLMå®¢æˆ·ç«¯ç¼“å­˜** (`src/core/llm/clients/`)
   - å„æä¾›å•†å®¢æˆ·ç«¯çš„å†…ç½®ç¼“å­˜æœºåˆ¶

### 2. Tokenç»Ÿè®¡æ¨¡å—åˆ†æ

#### APIå“åº”å¤„ç†èƒ½åŠ›

**OpenAIå¤„ç†å™¨** ([`OpenAITokenProcessor.parse_response`](src/services/llm/token_processing/openai_processor.py:130)):
```python
def parse_response(self, response: Dict[str, Any]) -> Optional[TokenUsage]:
    usage = response.get("usage", {})
    token_usage = TokenUsage(
        prompt_tokens=usage.get("prompt_tokens", 0),
        completion_tokens=usage.get("completion_tokens", 0),
        total_tokens=usage.get("total_tokens", 0),
        source="api",
        additional_info={
            "model": response.get("model"),
            "response_id": response.get("id"),
            "system_fingerprint": response.get("system_fingerprint")
        }
    )
```

**Geminiå¤„ç†å™¨** ([`GeminiTokenProcessor.parse_response`](src/services/llm/token_processing/gemini_processor.py:69)):
```python
def parse_response(self, response: Dict[str, Any]) -> Optional[TokenUsage]:
    metadata = response.get("usageMetadata", {})
    token_usage = TokenUsage(
        prompt_tokens=metadata.get("promptTokenCount", 0),
        completion_tokens=metadata.get("candidatesTokenCount", 0),
        total_tokens=metadata.get("totalTokenCount", 0),
        source="api",
        additional_info={
            "cached_tokens": metadata.get("cachedContentTokenCount", 0),
            # å…¶ä»–å…ƒæ•°æ®...
        }
    )
```

**Anthropicå¤„ç†å™¨** ([`AnthropicTokenProcessor.parse_response`](src/services/llm/token_processing/anthropic_processor.py:69)):
```python
def parse_response(self, response: Dict[str, Any]) -> Optional[TokenUsage]:
    usage = response.get("usage", {})
    token_usage = TokenUsage(
        prompt_tokens=usage.get("input_tokens", 0),
        completion_tokens=usage.get("output_tokens", 0),
        total_tokens=input_tokens + output_tokens,
        source="api",
        additional_info={
            "cache_creation_tokens": usage.get("cache_creation_input_tokens", 0),
            "cache_read_tokens": usage.get("cache_read_input_tokens", 0),
            # å…¶ä»–ç¼“å­˜ç›¸å…³ä¿¡æ¯...
        }
    )
```

## å†—ä½™æ€§åˆ†æ

### 1. æç¤ºè¯ç¼“å­˜çš„å†—ä½™æ€§

#### ğŸ”´ é«˜åº¦å†—ä½™çš„ç¼“å­˜å±‚

1. **Tokenå¤„ç†å™¨å±‚é¢çš„ç¼“å­˜**
   - **é—®é¢˜**: é¡¹ç›®ä»…è°ƒç”¨LLM APIï¼Œä¸ä½¿ç”¨LLMç¼“å­˜åŠŸèƒ½
   - **å†—ä½™ç¨‹åº¦**: ğŸ”´ æé«˜
   - **åŸå› **: 
     - [`BaseTokenProcessor._usage_cache`](src/services/llm/token_processing/base_processor.py:307) ç¼“å­˜APIå“åº”çš„tokenä½¿ç”¨æƒ…å†µ
     - ä½†é¡¹ç›®ä¸ä½¿ç”¨ç¼“å­˜ï¼Œæ¯æ¬¡éƒ½æ˜¯æ–°çš„APIè°ƒç”¨
     - ç¼“å­˜çš„é”®åŸºäºå†…å®¹å“ˆå¸Œï¼Œä½†APIå“åº”æ€»æ˜¯å”¯ä¸€çš„

2. **CacheManagerçš„å¤æ‚ç¼“å­˜ç­–ç•¥**
   - **é—®é¢˜**: å®ç°äº†client/server/hybridä¸‰ç§ç¼“å­˜ç­–ç•¥
   - **å†—ä½™ç¨‹åº¦**: ğŸ”´ æé«˜
   - **åŸå› **:
     - [`LLMCacheConfig`](src/core/llm/cache/cache_config.py:30) æ”¯æŒå¤æ‚çš„ç¼“å­˜é…ç½®
     - [`CacheManager`](src/core/llm/cache/cache_manager.py:15) å®ç°äº†å¤šå±‚ç¼“å­˜é€»è¾‘
     - ä½†é¡¹ç›®ä»…è°ƒç”¨APIï¼Œä¸ä½¿ç”¨ä»»ä½•ç¼“å­˜ç­–ç•¥

3. **GeminiæœåŠ¡å™¨ç«¯ç¼“å­˜**
   - **é—®é¢˜**: å®ç°äº†å®Œæ•´çš„æœåŠ¡å™¨ç«¯ç¼“å­˜ç®¡ç†
   - **å†—ä½™ç¨‹åº¦**: ğŸ”´ æé«˜
   - **åŸå› **:
     - [`GeminiServerCacheProvider`](src/core/llm/cache/providers/gemini_server_provider.py:12) æä¾›å®Œæ•´çš„ç¼“å­˜API
     - ä½†é¡¹ç›®ä¸ä½¿ç”¨Geminiçš„ç¼“å­˜åŠŸèƒ½

#### ğŸŸ¡ éƒ¨åˆ†å†—ä½™çš„ç»Ÿè®¡åŠŸèƒ½

1. **ç¼“å­˜å‘½ä¸­ç»Ÿè®¡**
   - **é—®é¢˜**: å¤šä¸ªæ¨¡å—éƒ½ç»Ÿè®¡ç¼“å­˜å‘½ä¸­ï¼Œä½†å®é™…æ°¸è¿œä¸ä¼šå‘½ä¸­
   - **å†—ä½™ç¨‹åº¦**: ğŸŸ¡ ä¸­ç­‰
   - **åŸå› **:
     - [`BaseTokenProcessor`](src/services/llm/token_processing/base_processor.py:301) ç»Ÿè®¡cache_hits/cache_misses
     - [`CacheManager`](src/core/llm/cache/cache_manager.py:48) ä¹Ÿæœ‰ç›¸åŒçš„ç»Ÿè®¡
     - ä½†ç”±äºä¸ä½¿ç”¨ç¼“å­˜ï¼Œè¿™äº›ç»Ÿè®¡æ°¸è¿œä¸º0

### 2. APIå“åº”ä¸­ç¼“å­˜ä¿¡æ¯çš„è·å–èƒ½åŠ›

#### âœ… æ”¯æŒç¼“å­˜ä¿¡æ¯çš„æä¾›å•†

1. **Gemini**
   - **ç¼“å­˜ä¿¡æ¯**: [`cachedContentTokenCount`](src/services/llm/token_processing/gemini_processor.py:93)
   - **è·å–èƒ½åŠ›**: âœ… å®Œå…¨æ”¯æŒ
   - **å®é™…ä»·å€¼**: âŒ æ— ä»·å€¼ï¼ˆé¡¹ç›®ä¸ä½¿ç”¨ç¼“å­˜ï¼‰

2. **Anthropic**
   - **ç¼“å­˜ä¿¡æ¯**: 
     - [`cache_creation_input_tokens`](src/services/llm/token_processing/anthropic_processor.py:100)
     - [`cache_read_input_tokens`](src/services/llm/token_processing/anthropic_processor.py:101)
   - **è·å–èƒ½åŠ›**: âœ… å®Œå…¨æ”¯æŒ
   - **å®é™…ä»·å€¼**: âŒ æ— ä»·å€¼ï¼ˆé¡¹ç›®ä¸ä½¿ç”¨ç¼“å­˜ï¼‰

#### âŒ ä¸æ”¯æŒç¼“å­˜ä¿¡æ¯çš„æä¾›å•†

1. **OpenAI**
   - **ç¼“å­˜ä¿¡æ¯**: æ— ç›´æ¥ç¼“å­˜å‘½ä¸­ä¿¡æ¯
   - **è·å–èƒ½åŠ›**: âŒ ä¸æ”¯æŒ
   - **æ›¿ä»£æ–¹æ¡ˆ**: é€šè¿‡`system_fingerprint`é—´æ¥æ¨æ–­

## å¿…è¦æ€§è¯„ä¼°

### 1. å½“å‰é¡¹ç›®å®é™…éœ€æ±‚

#### é¡¹ç›®ç‰¹ç‚¹
- **ä»…è°ƒç”¨LLM API**: ä¸ä½¿ç”¨ä»»ä½•LLMç¼“å­˜åŠŸèƒ½
- **Tokenç»Ÿè®¡éœ€æ±‚**: ä»…éœ€è¦å‡†ç¡®çš„tokenä½¿ç”¨ç»Ÿè®¡
- **æˆæœ¬æ§åˆ¶**: é€šè¿‡tokenç»Ÿè®¡æ§åˆ¶æˆæœ¬

#### å®é™…éœ€è¦çš„ç»„ä»¶
1. **Tokenè®¡ç®—**: âœ… å¿…éœ€
2. **APIå“åº”è§£æ**: âœ… å¿…éœ€  
3. **ç¼“å­˜åŠŸèƒ½**: âŒ ä¸éœ€è¦
4. **å¤æ‚ç¼“å­˜ç­–ç•¥**: âŒ ä¸éœ€è¦
5. **æœåŠ¡å™¨ç«¯ç¼“å­˜**: âŒ ä¸éœ€è¦

### 2. ç¼“å­˜ç»„ä»¶çš„ä»·å€¼è¯„ä¼°

| ç»„ä»¶ | å¿…è¦æ€§ | å½“å‰ä»·å€¼ | å»ºè®®æ“ä½œ |
|------|--------|----------|----------|
| Tokenå¤„ç†å™¨ç¼“å­˜ | âŒ ä¸å¿…è¦ | æ—  | ç§»é™¤ |
| CacheManager | âŒ ä¸å¿…è¦ | æ—  | ç§»é™¤ |
| MemoryCacheProvider | âŒ ä¸å¿…è¦ | æ—  | ç§»é™¤ |
| GeminiServerCacheProvider | âŒ ä¸å¿…è¦ | æ—  | ç§»é™¤ |
| ç¼“å­˜é…ç½® | âŒ ä¸å¿…è¦ | æ—  | ç§»é™¤ |
| ç¼“å­˜ç»Ÿè®¡ | âŒ ä¸å¿…è¦ | æ—  | ç§»é™¤ |

## ä¼˜åŒ–å»ºè®®

### 1. çŸ­æœŸä¼˜åŒ–ï¼ˆç«‹å³å¯è¡Œï¼‰

#### ç§»é™¤å†—ä½™ç¼“å­˜ç»„ä»¶

1. **ç®€åŒ–Tokenå¤„ç†å™¨**
```python
# ç§»é™¤ç¼“å­˜ç›¸å…³ä»£ç 
class BaseTokenProcessor(ITokenProcessor):
    def __init__(self, model_name: str, provider: str):
        self.model_name = model_name
        self.provider = provider
        self._last_usage: Optional[TokenUsage] = None
        self._encoding: Optional[EncodingProtocol] = None
        self._load_fallback_encoding()
        
        # ä¿ç•™åŸºç¡€ç»Ÿè®¡ï¼Œç§»é™¤ç¼“å­˜ç»Ÿè®¡
        self._stats = {
            "total_requests": 0,
            "successful_calculations": 0,
            "failed_calculations": 0,
            # ç§»é™¤: "cache_hits": 0,
            # ç§»é™¤: "cache_misses": 0,
            "degradation_events": 0
        }
        
        # ç§»é™¤ç¼“å­˜åŠŸèƒ½
        # self._usage_cache: Dict[str, TokenUsage] = {}
        # self._cache_size = 1000
```

2. **ç§»é™¤CacheManagerä¾èµ–**
```python
# ç®€åŒ–LLMå®¢æˆ·ç«¯ï¼Œç§»é™¤ç¼“å­˜ç®¡ç†å™¨
class BaseLLMClient:
    def __init__(self, config):
        self.config = config
        # ç§»é™¤: self.cache_manager = CacheManager(...)
```

3. **ç®€åŒ–é…ç½®æ–‡ä»¶**
```yaml
# ç§»é™¤ç¼“å­˜ç›¸å…³é…ç½®
llms:
  openai_group:
    # ç§»é™¤æ•´ä¸ªcache_configéƒ¨åˆ†
    parameters:
      temperature: 0.7
      max_tokens: 2000
      # ... å…¶ä»–å‚æ•°
```

#### ä¿ç•™æœ‰ä»·å€¼çš„APIå“åº”è§£æ

1. **å¢å¼ºTokenUsageç»“æ„**
```python
@dataclass
class TokenUsage:
    prompt_tokens: int = 0
    completion_tokens: int = 0
    total_tokens: int = 0
    source: str = "api"  # å›ºå®šä¸º"api"
    timestamp: Optional[datetime] = None
    additional_info: Optional[Dict[str, Any]] = None
    
    # æ·»åŠ ç¼“å­˜ä¿¡æ¯å­—æ®µï¼ˆä¸ºå°†æ¥æ‰©å±•é¢„ç•™ï¼‰
    cache_hit: bool = False
    cached_tokens: int = 0
```

### 2. ä¸­æœŸä¼˜åŒ–ï¼ˆéœ€è¦é‡æ„ï¼‰

#### ç»Ÿä¸€Tokenå¤„ç†æ¶æ„

1. **ç®€åŒ–å¤„ç†å™¨å±‚æ¬¡**
```
å½“å‰æ¶æ„:
BaseTokenProcessor â†’ HybridTokenProcessor â†’ å…·ä½“å¤„ç†å™¨

ä¼˜åŒ–å:
BaseTokenProcessor â†’ å…·ä½“å¤„ç†å™¨ (OpenAI/Gemini/Anthropic)
```

2. **ç§»é™¤HybridTokenProcessor**
```python
# ç›´æ¥ä½¿ç”¨å…·ä½“å¤„ç†å™¨ï¼Œä¸éœ€è¦æ··åˆå¤„ç†å™¨
class TokenCalculationService:
    def _get_processor_for_model(self, model_type: str, model_name: str):
        if model_type.lower() == "openai":
            return OpenAITokenProcessor(model_name)
        elif model_type.lower() == "gemini":
            return GeminiTokenProcessor(model_name)
        elif model_type.lower() == "anthropic":
            return AnthropicTokenProcessor(model_name)
```

3. **ç®€åŒ–ç»Ÿè®¡ç³»ç»Ÿ**
```python
# ç»Ÿä¸€çš„ç»Ÿè®¡æ¥å£
class TokenStatistics:
    def __init__(self):
        self._stats = {
            "total_requests": 0,
            "successful_calculations": 0,
            "failed_calculations": 0,
            "total_prompt_tokens": 0,
            "total_completion_tokens": 0,
            "total_tokens": 0
        }
    
    def update_from_usage(self, usage: TokenUsage):
        self._stats["total_requests"] += 1
        self._stats["successful_calculations"] += 1
        self._stats["total_prompt_tokens"] += usage.prompt_tokens
        self._stats["total_completion_tokens"] += usage.completion_tokens
        self._stats["total_tokens"] += usage.total_tokens
```

### 3. é•¿æœŸä¼˜åŒ–ï¼ˆæ¶æ„æ”¹è¿›ï¼‰

#### æ¨¡å—åŒ–é‡æ„

1. **åˆ†ç¦»å…³æ³¨ç‚¹**
```
src/services/llm/
â”œâ”€â”€ token_processing/          # ä¿ç•™å¹¶ç®€åŒ–
â”‚   â”œâ”€â”€ base_processor.py     # ç®€åŒ–ç‰ˆæœ¬
â”‚   â”œâ”€â”€ openai_processor.py   # ä¿ç•™
â”‚   â”œâ”€â”€ gemini_processor.py   # ä¿ç•™
â”‚   â”œâ”€â”€ anthropic_processor.py # ä¿ç•™
â”‚   â””â”€â”€ token_types.py        # ä¿ç•™
â”œâ”€â”€ calculation_service.py    # ç®€åŒ–ç‰ˆæœ¬
â””â”€â”€ statistics.py             # æ–°å¢ç»Ÿä¸€ç»Ÿè®¡
```

2. **ç§»é™¤ç¼“å­˜æ¨¡å—**
```
ç§»é™¤:
â”œâ”€â”€ src/core/llm/cache/       # æ•´ä¸ªç›®å½•
â”œâ”€â”€ src/services/llm/token_processing/hybrid_processor.py
â””â”€â”€ é…ç½®æ–‡ä»¶ä¸­çš„ç¼“å­˜ç›¸å…³éƒ¨åˆ†
```

3. **ç®€åŒ–ä¾èµ–å…³ç³»**
```python
# å½“å‰å¤æ‚ä¾èµ–
TokenProcessor â†’ CacheManager â†’ MemoryProvider â†’ ServerProvider

# ä¼˜åŒ–åç®€å•ä¾èµ–
TokenProcessor â†’ EncodingProtocol
```

## å®æ–½å»ºè®®

### 1. ä¼˜å…ˆçº§æ’åº

#### ğŸ”´ é«˜ä¼˜å…ˆçº§ï¼ˆç«‹å³å®æ–½ï¼‰
1. **ç§»é™¤CacheManagerç›¸å…³ä»£ç **
   - åˆ é™¤ [`src/core/llm/cache/`](src/core/llm/cache/) æ•´ä¸ªç›®å½•
   - ç§»é™¤æ‰€æœ‰ç¼“å­˜ç®¡ç†å™¨çš„å¼•ç”¨

2. **ç®€åŒ–Tokenå¤„ç†å™¨**
   - ç§»é™¤ [`BaseTokenProcessor`](src/services/llm/token_processing/base_processor.py:307) ä¸­çš„ç¼“å­˜é€»è¾‘
   - ç§»é™¤ [`HybridTokenProcessor`](src/services/llm/token_processing/hybrid_processor.py) çš„ç¼“å­˜åŠŸèƒ½

3. **æ¸…ç†é…ç½®æ–‡ä»¶**
   - ç§»é™¤æ‰€æœ‰ `cache_config` ç›¸å…³é…ç½®
   - ç®€åŒ–LLMé…ç½®ç»“æ„

#### ğŸŸ¡ ä¸­ä¼˜å…ˆçº§ï¼ˆ1-2å‘¨å†…ï¼‰
1. **é‡æ„Tokenå¤„ç†æ¶æ„**
   - ç§»é™¤ [`HybridTokenProcessor`](src/services/llm/token_processing/hybrid_processor.py)
   - ç®€åŒ– [`TokenCalculationService`](src/services/llm/token_calculation_service.py)

2. **ç»Ÿä¸€ç»Ÿè®¡ç³»ç»Ÿ**
   - åˆ›å»ºç»Ÿä¸€çš„ç»Ÿè®¡ç±»
   - ç§»é™¤é‡å¤çš„ç»Ÿè®¡é€»è¾‘

#### ğŸŸ¢ ä½ä¼˜å…ˆçº§ï¼ˆé•¿æœŸè§„åˆ’ï¼‰
1. **æ€§èƒ½ä¼˜åŒ–**
   - ä¼˜åŒ–tokenè®¡ç®—æ€§èƒ½
   - å‡å°‘ä¸å¿…è¦çš„å¯¹è±¡åˆ›å»º

2. **ç›‘æ§å¢å¼º**
   - æ·»åŠ æ›´è¯¦ç»†çš„æ€§èƒ½ç›‘æ§
   - ä¼˜åŒ–é”™è¯¯å¤„ç†

### 2. é£é™©è¯„ä¼°

#### é‡æ„é£é™©
- **åŠŸèƒ½å›å½’é£é™©**: ç§»é™¤ç¼“å­˜å¯èƒ½å½±å“æœªæ¥çš„ç¼“å­˜éœ€æ±‚
- **å…¼å®¹æ€§é£é™©**: ç°æœ‰ä»£ç å¯èƒ½ä¾èµ–ç¼“å­˜æ¥å£
- **æµ‹è¯•è¦†ç›–é£é™©**: éœ€è¦ç¡®ä¿ç§»é™¤ç¼“å­˜ååŠŸèƒ½æ­£å¸¸

#### ç¼“è§£æªæ–½
- **æ¸è¿›å¼é‡æ„**: åˆ†é˜¶æ®µç§»é™¤ç¼“å­˜åŠŸèƒ½
- **ä¿ç•™æ¥å£**: ä¿ç•™ç¼“å­˜æ¥å£ä½†æä¾›ç©ºå®ç°
- **å……åˆ†æµ‹è¯•**: ç¡®ä¿æ‰€æœ‰åŠŸèƒ½æ­£å¸¸å·¥ä½œ

### 3. å®æ–½æ­¥éª¤

#### ç¬¬ä¸€é˜¶æ®µï¼šç§»é™¤ç¼“å­˜å®ç°
1. å¤‡ä»½å½“å‰ä»£ç 
2. ç§»é™¤ [`src/core/llm/cache/`](src/core/llm/cache/) ç›®å½•
3. æ›´æ–°æ‰€æœ‰å¼•ç”¨
4. è¿è¡Œæµ‹è¯•ç¡®ä¿åŠŸèƒ½æ­£å¸¸

#### ç¬¬äºŒé˜¶æ®µï¼šç®€åŒ–Tokenå¤„ç†å™¨
1. ç§»é™¤ [`BaseTokenProcessor`](src/services/llm/token_processing/base_processor.py) ä¸­çš„ç¼“å­˜é€»è¾‘
2. ç§»é™¤ [`HybridTokenProcessor`](src/services/llm/token_processing/hybrid_processor.py)
3. æ›´æ–° [`TokenCalculationService`](src/services/llm/token_calculation_service.py)
4. æµ‹è¯•tokenè®¡ç®—åŠŸèƒ½

#### ç¬¬ä¸‰é˜¶æ®µï¼šæ¸…ç†é…ç½®
1. ç§»é™¤é…ç½®æ–‡ä»¶ä¸­çš„ç¼“å­˜é…ç½®
2. æ›´æ–°æ–‡æ¡£
3. éªŒè¯é…ç½®åŠ è½½

## ç»“è®º

### 1. å†—ä½™æ€§æ€»ç»“

å½“å‰é¡¹ç›®çš„æç¤ºè¯ç¼“å­˜ç³»ç»Ÿå­˜åœ¨**æé«˜çš„å†—ä½™æ€§**ï¼š

- **ğŸ”´ ç¼“å­˜å®ç°**: 100%å†—ä½™ï¼Œé¡¹ç›®ä¸ä½¿ç”¨ä»»ä½•ç¼“å­˜åŠŸèƒ½
- **ğŸ”´ ç¼“å­˜ç­–ç•¥**: 100%å†—ä½™ï¼Œå¤æ‚çš„client/server/hybridç­–ç•¥æ— å®é™…ç”¨é€”
- **ğŸ”´ ç¼“å­˜ç»Ÿè®¡**: 100%å†—ä½™ï¼Œç¼“å­˜å‘½ä¸­ç»Ÿè®¡æ°¸è¿œä¸º0
- **ğŸŸ¡ APIå“åº”è§£æ**: éƒ¨åˆ†æœ‰ä»·å€¼ï¼Œä½†ç¼“å­˜ç›¸å…³å­—æ®µæ— æ„ä¹‰

### 2. ä¼˜åŒ–æ”¶ç›Š

#### ä»£ç ç®€åŒ–
- **å‡å°‘ä»£ç é‡**: é¢„è®¡å‡å°‘30-40%çš„LLMç›¸å…³ä»£ç 
- **é™ä½å¤æ‚åº¦**: ç§»é™¤å¤šå±‚ç¼“å­˜æ¶æ„
- **æé«˜å¯ç»´æŠ¤æ€§**: ç®€åŒ–ä¾èµ–å…³ç³»

#### æ€§èƒ½æå‡
- **å‡å°‘å†…å­˜ä½¿ç”¨**: ç§»é™¤ç¼“å­˜æ•°æ®ç»“æ„
- **å‡å°‘CPUå¼€é”€**: ç§»é™¤ç¼“å­˜æŸ¥æ‰¾é€»è¾‘
- **åŠ å¿«å¯åŠ¨é€Ÿåº¦**: å‡å°‘åˆå§‹åŒ–ç»„ä»¶

#### ç»´æŠ¤æˆæœ¬é™ä½
- **å‡å°‘æµ‹è¯•å¤æ‚åº¦**: ç§»é™¤ç¼“å­˜ç›¸å…³æµ‹è¯•
- **é™ä½è°ƒè¯•éš¾åº¦**: ç®€åŒ–è°ƒç”¨é“¾
- **å‡å°‘æ–‡æ¡£ç»´æŠ¤**: ç§»é™¤ç¼“å­˜ç›¸å…³æ–‡æ¡£

### 3. æœ€ç»ˆå»ºè®®

**å¼ºçƒˆå»ºè®®ç§»é™¤æ•´ä¸ªæç¤ºè¯ç¼“å­˜ç³»ç»Ÿ**ï¼ŒåŸå› å¦‚ä¸‹ï¼š

1. **å®Œå…¨æ— å®é™…ç”¨é€”**: é¡¹ç›®ä»…è°ƒç”¨APIï¼Œä¸ä½¿ç”¨ç¼“å­˜
2. **æé«˜çš„å†—ä½™æ€§**: æ‰€æœ‰ç¼“å­˜ç»„ä»¶éƒ½æ˜¯å¤šä½™çš„
3. **ç»´æŠ¤æˆæœ¬é«˜**: å¤æ‚çš„ç¼“å­˜æ¶æ„å¢åŠ ç»´æŠ¤è´Ÿæ‹…
4. **æ€§èƒ½å½±å“**: ä¸å¿…è¦çš„ç¼“å­˜æ“ä½œå½±å“æ€§èƒ½

**ä¿ç•™çš„æ ¸å¿ƒç»„ä»¶**ï¼š
- Tokenè®¡ç®—åŠŸèƒ½
- APIå“åº”è§£æåŠŸèƒ½  
- åŸºç¡€ç»Ÿè®¡åŠŸèƒ½
- å„æä¾›å•†çš„ç‰¹å®šå¤„ç†å™¨

**ç§»é™¤çš„å†—ä½™ç»„ä»¶**ï¼š
- æ‰€æœ‰ç¼“å­˜å®ç°
- ç¼“å­˜ç®¡ç†å™¨
- ç¼“å­˜é…ç½®
- ç¼“å­˜ç»Ÿè®¡
- æ··åˆå¤„ç†å™¨

é€šè¿‡è¿™æ¬¡ä¼˜åŒ–ï¼Œé¡¹ç›®å°†è·å¾—æ›´ç®€æ´ã€æ›´é«˜æ•ˆã€æ›´æ˜“ç»´æŠ¤çš„LLMé›†æˆæ¶æ„ã€‚