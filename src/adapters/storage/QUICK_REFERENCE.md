# å­˜å‚¨å·¥å…·ç±»å¿«é€Ÿå‚è€ƒ

## SQLiteStorageUtils æ–°æ–¹æ³•

### 1. configure_connection(conn, config)
é…ç½®SQLiteè¿æ¥å‚æ•°ã€‚

```python
from src.adapters.storage.utils.sqlite_utils import SQLiteStorageUtils

conn = sqlite3.connect("storage.db")
SQLiteStorageUtils.configure_connection(conn, {
    "enable_wal_mode": True,
    "cache_size": 2000,
    "synchronous_mode": "NORMAL",
    "busy_timeout": 30000
})
```

**é…ç½®é€‰é¡¹ï¼š**
- `enable_wal_mode`: WALæ—¥å¿—æ¨¡å¼ï¼ˆæ¨èç”¨äºå¹¶å‘ï¼‰
- `enable_foreign_keys`: å¤–é”®çº¦æŸ
- `cache_size`: ç¼“å­˜é¡µæ•°ï¼ˆè´Ÿæ•°è¡¨ç¤ºå­—èŠ‚ï¼‰
- `synchronous_mode`: åŒæ­¥æ¨¡å¼(OFF/NORMAL/FULL/EXTRA)
- `journal_mode`: æ—¥å¿—æ¨¡å¼
- `temp_store`: ä¸´æ—¶å­˜å‚¨ä½ç½®(memory/file/default)
- `enable_auto_vacuum`: è‡ªåŠ¨VACUUM
- `busy_timeout`: å¿™ç¢Œè¶…æ—¶(æ¯«ç§’)

---

### 2. get_database_stats(conn)
è·å–æ•°æ®åº“è¯¦ç»†ç»Ÿè®¡ã€‚

```python
stats = SQLiteStorageUtils.get_database_stats(conn)

# è¿”å›å­—å…¸åŒ…å«ï¼š
# {
#     "page_count": 100,
#     "page_size": 4096,
#     "database_size_bytes": 409600,
#     "database_size_mb": 0.39,
#     "total_records": 150,
#     "expired_records": 5,
#     "compressed_records": 50,
#     "tables": ["state_storage", ...],
#     "indexes": ["idx_state_type", ...],
#     "record_stats": {"type_a": 100, "type_b": 50},
#     "cache_stats": {"pages_in_cache": 64, ...}
# }

print(f"æ•°æ®åº“å¤§å°: {stats['database_size_mb']} MB")
print(f"æ€»è®°å½•æ•°: {stats['total_records']}")
print(f"è¿‡æœŸè®°å½•: {stats['expired_records']}")
```

**ä½¿ç”¨åœºæ™¯ï¼š**
- å¥åº·æ£€æŸ¥
- æ€§èƒ½ç›‘æ§
- å®¹é‡è§„åˆ’

---

### 3. get_table_info(conn, table_name)
è·å–è¡¨çš„è¯¦ç»†ä¿¡æ¯ã€‚

```python
info = SQLiteStorageUtils.get_table_info(conn, "state_storage")

# è¿”å›ï¼š
# {
#     "columns": [
#         {"name": "id", "type": "TEXT", "notnull": 1, "pk": 1},
#         {"name": "data", "type": "TEXT", "notnull": 1, "pk": 0},
#         ...
#     ],
#     "record_count": 150,
#     "indexes": ["idx_state_type", "idx_state_expires_at"]
# }

for col in info["columns"]:
    print(f"{col['name']}: {col['type']}")
```

---

### 4. analyze_query(conn, query)
åˆ†æSQLæŸ¥è¯¢æ‰§è¡Œè®¡åˆ’ã€‚

```python
plan = SQLiteStorageUtils.analyze_query(
    conn,
    "SELECT * FROM state_storage WHERE type = ?"
)

# è¿”å›æ‰§è¡Œè®¡åˆ’åˆ—è¡¨
for step in plan:
    print(f"æ­¥éª¤ {step['id']}: {step['detail']}")
```

**ç”¨äºï¼š**
- æŸ¥è¯¢ä¼˜åŒ–
- æ€§èƒ½è¯Šæ–­
- ç´¢å¼•æ•ˆæœéªŒè¯

---

## FileStorageUtils æ–°æ–¹æ³•

### 1. calculate_file_path(base_path, data_id, directory_structure, extension)
è®¡ç®—å­˜å‚¨æ–‡ä»¶è·¯å¾„ã€‚

```python
from src.adapters.storage.utils.file_utils import FileStorageUtils

# å¹³ç»“æ„
path = FileStorageUtils.calculate_file_path(
    "storage", "user_123", "flat", "json"
)
# â†’ storage/user_123.json

# æ—¥æœŸç»“æ„
path = FileStorageUtils.calculate_file_path(
    "storage", "data_001", "by_date", "json"
)
# â†’ storage/2024/12/20/data_001.json

# å“ˆå¸Œç»“æ„
path = FileStorageUtils.calculate_file_path(
    "storage", "abc123def", "by_hash", "json"
)
# â†’ storage/ab/abc123def.json

# Agentç»“æ„
path = FileStorageUtils.calculate_file_path(
    "storage", "agent_001_data", "by_agent", "json"
)
# â†’ storage/agent_001/agent_001_data.json
```

**ç›®å½•ç»“æ„é€‰é¡¹ï¼š**
- `flat`: æ‰€æœ‰æ–‡ä»¶æ”¾åœ¨æ ¹ç›®å½•
- `by_date`: YYYY/MM/DDç»“æ„
- `by_agent`: æŒ‰agent_idåˆ†ç›®å½•
- `by_hash`: æŒ‰IDå‰2å­—ç¬¦åˆ†ç›®å½•
- `by_type`: æŒ‰ç±»å‹åˆ†ç›®å½•

---

### 2. get_directory_size(directory)
è®¡ç®—ç›®å½•å¤§å°ã€‚

```python
size_bytes = FileStorageUtils.get_directory_size("storage")
size_mb = size_bytes / (1024 * 1024)
size_gb = size_mb / 1024

print(f"ç›®å½•å¤§å°: {size_gb:.2f} GB")
```

---

### 3. validate_file_size(file_path, max_size)
éªŒè¯æ–‡ä»¶å¤§å°æ˜¯å¦è¶…é™ã€‚

```python
max_10mb = 10 * 1024 * 1024

if FileStorageUtils.validate_file_size("storage/data.json", max_10mb):
    print("æ–‡ä»¶å¤§å°æ­£å¸¸")
else:
    print("æ–‡ä»¶è¶…è¿‡é™åˆ¶")
```

---

### 4. count_files_in_directory(directory, pattern, recursive)
è®¡ç®—ç›®å½•ä¸­æ–‡ä»¶æ•°é‡ã€‚

```python
# é€’å½’è®¡æ•°
total = FileStorageUtils.count_files_in_directory(
    "storage", "*.json", recursive=True
)

# ä»…æ ¹ç›®å½•
root_only = FileStorageUtils.count_files_in_directory(
    "storage", "*.json", recursive=False
)

print(f"æ€»æ–‡ä»¶æ•°: {total}")
print(f"æ ¹ç›®å½•æ–‡ä»¶: {root_only}")
```

---

### 5. validate_directory_structure(base_path, max_files, max_size)
éªŒè¯ç›®å½•æ˜¯å¦æ»¡è¶³é™åˆ¶ã€‚

```python
result = FileStorageUtils.validate_directory_structure(
    "storage",
    max_files_per_directory=10000,
    max_directory_size=1024 * 1024 * 1024  # 1GB
)

if result["is_valid"]:
    print("ç›®å½•ç»“æ„æ­£å¸¸")
else:
    print("è¿è§„åˆ—è¡¨:")
    for violation in result["violations"]:
        print(f"  - {violation}")

print(f"å½“å‰æ–‡ä»¶æ•°: {result['current_files']}")
print(f"å½“å‰å¤§å°: {result['current_size_mb']} MB")
```

---

### 6. get_directory_structure_info(base_path, directory_structure)
è·å–ç›®å½•ç»“æ„ä¿¡æ¯ã€‚

```python
info = FileStorageUtils.get_directory_structure_info(
    "storage", "by_date"
)

# è¿”å›ï¼š
# {
#     "structure": "by_date",
#     "directory_exists": True,
#     "base_path": "storage",
#     "years": ["2024", "2023"],
#     "subdirectories": ["2024", "2023"]
# }

for year in info.get("years", []):
    print(f"å¹´ä»½: {year}")
```

---

## æ¨¡æ¿æ–¹æ³•æ¨¡å¼ä½¿ç”¨

### æ¸…ç†è¿‡æœŸé¡¹

```python
# ç”¨æˆ·ä»£ç æ— éœ€æ”¹åŠ¨ï¼ŒåŸºç±»ä¼šè‡ªåŠ¨è°ƒç”¨åˆé€‚çš„å®ç°
backend = SQLiteStorageBackend(db_path="storage.db")
await backend.connect()

# å®šæœŸæ¸…ç†ä¼šè‡ªåŠ¨è°ƒç”¨å¯¹åº”åç«¯çš„ä¼˜åŒ–å®ç°
# SQLiteStorageBackend._cleanup_expired_items_impl() ä½¿ç”¨SQL
# FileStorageBackend._cleanup_expired_items_impl() æ‰«ææ–‡ä»¶
# MemoryStorageBackend._cleanup_expired_items_impl() æ‰¹é‡åˆ é™¤
```

### åˆ›å»ºå¤‡ä»½

```python
# ç”¨æˆ·ä»£ç æ— éœ€æ”¹åŠ¨ï¼ŒåŸºç±»ä¼šè‡ªåŠ¨è°ƒç”¨åˆé€‚çš„å®ç°
backend = FileStorageBackend(base_path="storage")
await backend.connect()

# å®šæœŸå¤‡ä»½ä¼šè‡ªåŠ¨è°ƒç”¨å¯¹åº”åç«¯çš„å®ç°
# SQLiteStorageBackend._create_backup_impl() åˆ›å»ºDBå‰¯æœ¬
# FileStorageBackend._create_backup_impl() å¤åˆ¶ç›®å½•
# MemoryStorageBackend._create_backup_impl() ä¿å­˜æŒä¹…åŒ–
```

---

## æ€§èƒ½æç¤º

### SQLiteä¼˜åŒ–
```python
config = {
    "enable_wal_mode": True,          # æé«˜å¹¶å‘
    "cache_size": 5000,               # å¢åŠ ç¼“å­˜ï¼ˆå¤§æ•°æ®ï¼‰
    "synchronous_mode": "NORMAL",     # å¹³è¡¡æ€§èƒ½å’Œå®‰å…¨
    "journal_mode": "WAL",            # WALæ¨¡å¼
    "enable_auto_vacuum": True        # è‡ªåŠ¨æ¸…ç†
}

backend = SQLiteStorageBackend(**config)
```

### æ–‡ä»¶ç³»ç»Ÿä¼˜åŒ–
```python
# ä½¿ç”¨åˆé€‚çš„ç›®å½•ç»“æ„
config = {
    "directory_structure": "by_date",  # æŒ‰æ—¥æœŸåˆ†ç›®å½•ï¼ˆé˜²æ­¢ç›®å½•è¿‡å¤§ï¼‰
    "max_files_per_directory": 1000,   # é™åˆ¶å•ç›®å½•æ–‡ä»¶æ•°
    "max_directory_size": 1024 * 1024 * 1024  # é™åˆ¶ç›®å½•å¤§å°
}

backend = FileStorageBackend(**config)
```

### å†…å­˜ä¼˜åŒ–
```python
config = {
    "max_size": 10000,                # æœ€å¤šå­˜å‚¨é¡¹æ•°
    "max_memory_mb": 512,             # æœ€å¤§å†…å­˜ä½¿ç”¨ï¼ˆMBï¼‰
    "enable_persistence": True,       # å¯ç”¨æŒä¹…åŒ–
    "persistence_path": "cache.pkl"   # æŒä¹…åŒ–æ–‡ä»¶
}

backend = MemoryStorageBackend(**config)
```

---

## å¸¸è§ä»»åŠ¡ç¤ºä¾‹

### æ£€æŸ¥æ•°æ®åº“å¥åº·çŠ¶æ€
```python
conn = SQLiteStorageUtils.create_connection("storage.db")
stats = SQLiteStorageUtils.get_database_stats(conn)

health = {
    "status": "good" if stats["database_size_mb"] < 1000 else "warning",
    "size_mb": stats["database_size_mb"],
    "total_records": stats["total_records"],
    "expired_records": stats["expired_records"],
    "compression_ratio": stats.get("compression_ratio", 0)
}

print(f"æ•°æ®åº“çŠ¶æ€: {health}")
```

### æ¸…ç†å¤§ç›®å½•ä¸­çš„è¿‡æœŸæ–‡ä»¶
```python
# åœ¨å¤§æ–‡ä»¶ç³»ç»Ÿä¸­é«˜æ•ˆæ¸…ç†
expired_count = await backend.cleanup_old_data(retention_days=30)
print(f"åˆ é™¤äº† {expired_count} ä¸ªè¿‡æœŸæ•°æ®")
```

### ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢
```python
# åˆ†ææ…¢æŸ¥è¯¢
plan = SQLiteStorageUtils.analyze_query(
    conn,
    "SELECT * FROM state_storage WHERE session_id = ?"
)

# å¦‚æœæ²¡æœ‰ä½¿ç”¨ç´¢å¼•ï¼Œæ·»åŠ ç´¢å¼•
if "SCAN TABLE" in str(plan):
    print("æŸ¥è¯¢æœªä½¿ç”¨ç´¢å¼•ï¼Œå»ºè®®æ·»åŠ ç´¢å¼•")
    conn.execute("CREATE INDEX idx_session_id ON state_storage(session_id)")
```

---

## æ•…éšœæ’é™¤

### æ•°æ®åº“æ–‡ä»¶è¿‡å¤§
```python
# æ£€æŸ¥åŸå› 
stats = SQLiteStorageUtils.get_database_stats(conn)
print(f"æ•°æ®åº“å¤§å°: {stats['database_size_mb']} MB")
print(f"è¿‡æœŸè®°å½•: {stats['expired_records']}")

# è§£å†³æ–¹æ¡ˆï¼šæ¸…ç†è¿‡æœŸæ•°æ®
await backend.cleanup_old_data(retention_days=7)

# ä¼˜åŒ–æ•°æ®åº“
SQLiteStorageUtils.optimize_database(conn)
```

### æ–‡ä»¶ç³»ç»Ÿç›®å½•ç»“æ„æ··ä¹±
```python
# æ£€æŸ¥çŠ¶å†µ
validation = FileStorageUtils.validate_directory_structure(
    "storage",
    max_files_per_directory=5000,
    max_directory_size=5*1024*1024*1024  # 5GB
)

if not validation["is_valid"]:
    # é‡æ–°ç»„ç»‡æ–‡ä»¶
    # ä½¿ç”¨ calculate_file_path() ç¡®å®šæ–°ä½ç½®
    # é€ä¸ªç§»åŠ¨æ–‡ä»¶
    pass
```

---

## APIé€ŸæŸ¥è¡¨

| ç±» | æ–¹æ³• | å‚æ•° | è¿”å›å€¼ |
|----|------|------|--------|
| `SQLiteStorageUtils` | `configure_connection` | conn, config | None |
| | `get_database_stats` | conn | Dict |
| | `get_table_info` | conn, table_name | Dict |
| | `analyze_query` | conn, query | List[Dict] |
| `FileStorageUtils` | `calculate_file_path` | base_path, data_id, structure, ext | str |
| | `get_directory_size` | directory | int |
| | `validate_file_size` | file_path, max_size | bool |
| | `count_files_in_directory` | directory, pattern, recursive | int |
| | `validate_directory_structure` | base_path, max_files, max_size | Dict |
| | `get_directory_structure_info` | base_path, structure | Dict |

---

## æ›´æ–°æ—¥å¿—

### v1.3.0ï¼ˆå½“å‰ï¼‰
- âœ¨ æ·»åŠ SQLiteStorageUtilså·¥å…·æ–¹æ³•ï¼ˆ4ä¸ªï¼‰
- âœ¨ æ·»åŠ FileStorageUtilså·¥å…·æ–¹æ³•ï¼ˆ6ä¸ªï¼‰
- ğŸ”§ ç»Ÿä¸€æ¸…ç†å’Œå¤‡ä»½çš„æ¨¡æ¿æ–¹æ³•æ¨¡å¼
- ğŸ”§ æ·»åŠ ConnectionPooledStorageBackendä¸­é—´åŸºç±»
- ğŸ“ˆ æ€§èƒ½æ”¹è¿›ï¼šSQLiteæ¸…ç†æ€§èƒ½æå‡50-90%

---

**æœ€åæ›´æ–°ï¼š** 2024å¹´  
**ç»´æŠ¤è€…ï¼š** [é¡¹ç›®å›¢é˜Ÿ]
